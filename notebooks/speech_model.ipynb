{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import librosa\n",
    "from python_speech_features import mfcc as psf_mfcc\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tiktoken\n",
    "import json\n",
    "import os\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, filename=\"model_test.log\", filemode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_mfcc(audio_path, n_mfcc=13):\n",
    "#     y, sr = librosa.load(audio_path, sr=None)\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "#     mfcc = torch.tensor(mfcc).unsqueeze(0)  # Add batch dimension\n",
    "#     return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, audio_paths, transcripts, target_num_frames=100, transform=None):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.transcripts = transcripts\n",
    "        self.target_num_frames = target_num_frames\n",
    "        self.transform = transform\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(transcripts)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        transcript = self.transcripts[idx]\n",
    "        audio_data, sample_rate = sf.read(audio_path)\n",
    "        if self.transform:\n",
    "            mfcc = self.transform(audio_path)\n",
    "        else:\n",
    "            mfcc = psf_mfcc(audio_data, samplerate=sample_rate, numcep=13)\n",
    "            mfccs_normalized = (mfcc - np.mean(mfcc, axis=0)) / np.std(mfcc, axis=0)\n",
    "        \n",
    "        # Ensure equal number of frames\n",
    "        if mfccs_normalized.shape[0] < self.target_num_frames:\n",
    "            # Pad with zeros\n",
    "            padding = np.zeros((self.target_num_frames - mfccs_normalized.shape[0], mfccs_normalized.shape[1]))\n",
    "            mfccs_normalized = np.vstack((mfccs_normalized, padding))\n",
    "        elif mfccs_normalized.shape[0] > self.target_num_frames:\n",
    "            # Truncate\n",
    "            mfccs_normalized = mfccs_normalized[:self.target_num_frames, :]\n",
    "        \n",
    "        # Adding a channel dimension for CNN input: (num_frames, num_mfcc_coeffs) -> (1, num_frames, num_mfcc_coeffs)\n",
    "        mfccs_normalized = mfccs_normalized[np.newaxis, ...]\n",
    "        \n",
    "        label = self.label_encoder.transform([transcript])[0]\n",
    "        \n",
    "        return torch.tensor(mfccs_normalized, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return len(self.label_encoder.classes_)\n",
    "    \n",
    "    def decode(self, encoded_label):\n",
    "        return self.label_encoder.inverse_transform([encoded_label])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBiLSTM(nn.Module):\n",
    "    def __init__(self, num_classes, num_frames):\n",
    "        super(CNNBiLSTM, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (batch_size, 32, num_frames//2, num_features//2)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),   # Output: (batch_size, 64, num_frames//4, num_features//4)\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Compute the flattened size after CNN layers\n",
    "        num_features = 13\n",
    "        cnn_output_size = 64 * (num_frames // 4) * (num_features // 4)\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_size=cnn_output_size, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(128 * 2, num_classes)\n",
    "        self.layer_norm = nn.LayerNorm(128 * 2)  # Apply LayerNorm after LSTM output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.unsqueeze(1)  # Add time dimension for LSTM\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        counter+=1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # logging.info(f\"data and labels loaded to device for dataset: {counter} in the minibatch\")\n",
    "        optimizer.zero_grad()\n",
    "        # logging.info(f\"grad set to zero for minibatch: {counter}\")\n",
    "        outputs = model(inputs)\n",
    "        # logging.info(f\"model output calculated for minibatch: {counter}\")\n",
    "        loss = criterion(outputs, labels)\n",
    "        # logger.info(f\"Loss calculated for minibatch: {counter}\")\n",
    "        loss.backward()\n",
    "        # logging.info(f\"grad calculated for minibatch: {counter}\")\n",
    "        optimizer.step()\n",
    "        # logging.info(f\"weights changed for minibatch: {counter}\")\n",
    "        running_loss += loss.item()\n",
    "        if counter%50==0:\n",
    "            logging.info(f\"ready for next minibatch {counter}\")\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(mode=\"train\"):\n",
    "    corpus_path = r\"data\\processed\"\n",
    "    curr_notebook_dir = os.getcwd()\n",
    "    parent_dir = os.path.abspath(os.path.join(curr_notebook_dir,os.pardir))\n",
    "    corpus_path_whole = os.path.join(parent_dir,corpus_path)\n",
    "    \n",
    "    mode_path = mode+\"_corpus_new.json\"\n",
    "    \n",
    "    corpus_path_mode = os.path.join(corpus_path_whole, mode_path)\n",
    "    \n",
    "    with open(corpus_path_mode, \"r\") as f:\n",
    "        mode_corpus = json.load(f)\n",
    "    \n",
    "    return mode_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_corpus:list[dict], val_corpus:list[dict], test_corpus:list[dict]):\n",
    "    \n",
    "    num_frames = 1000\n",
    "    print(\"Generating train dataset\")\n",
    "    \n",
    "    train_audio_path, train_transcript = [],[]\n",
    "    for files in train_corpus:\n",
    "        train_audio_path.append(files[\"file\"])\n",
    "        train_transcript.append(files[\"transcript\"])\n",
    "    train_dataset = SpeechDataset(train_audio_path, train_transcript, target_num_frames=num_frames)\n",
    "    \n",
    "        \n",
    "    num_classes = train_dataset.get_num_classes()\n",
    "    \n",
    "    print(\"Generating validation dataset\")\n",
    "    \n",
    "    val_audio_path, val_transcript = [],[]\n",
    "    for files in val_corpus:\n",
    "        val_audio_path.append(files[\"file\"])\n",
    "        val_transcript.append(files[\"transcript\"])\n",
    "    val_dataset = SpeechDataset(val_audio_path, val_transcript, target_num_frames=num_frames)\n",
    "    \n",
    "    print(\"Generating test dataset\")\n",
    "    \n",
    "    test_audio_path, test_transcript = [],[]\n",
    "    for files in test_corpus:\n",
    "        test_audio_path.append(files[\"file\"])\n",
    "        test_transcript.append(files[\"transcript\"])\n",
    "    test_dataset = SpeechDataset(test_audio_path, test_transcript, target_num_frames=num_frames)\n",
    "    \n",
    "    batch_size = 64\n",
    "    \n",
    "    # train_size = 10000\n",
    "    # train_sampler = SubsetRandomSampler(np.arange(train_size))\n",
    "    # train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Model loading: \")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNBiLSTM(num_classes=num_classes, num_frames = num_frames).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    print(\"training start:\")\n",
    "    \n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        logging.info(f\"entered epoch: {epoch}\")\n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        logging.info(f\"train loss calculated for epoch {epoch}:\")\n",
    "        val_loss = validate(model, val_dataloader, criterion, device)\n",
    "        logging.info(f\"val loss calculated for epoch {epoch}:\")\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    test_loss = validate(model, test_dataloader, criterion, device)\n",
    "    print(\"------------------------------------\")\n",
    "    print(f'Final Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model, train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(test_audio, model, train_dataset):\n",
    "    # Create a SpeechDataset instance for the single test audio\n",
    "    test_audio_path = [test_audio[\"file\"]]\n",
    "    test_transcript = [test_audio[\"transcript\"]]\n",
    "    test_dataset = SpeechDataset(test_audio_path, test_transcript)\n",
    "    \n",
    "    \n",
    "    inputs, label = test_dataset[0]\n",
    "    inputs = inputs.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(next(model.parameters()).device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.argmax(outputs, 1).item()\n",
    "    \n",
    "    predicted_transcript = train_dataset.decode(predicted)\n",
    "    \n",
    "    print(f\"Original transcript: {test_transcript[0]}\")\n",
    "    print(f\"Predicted transcript: {predicted_transcript}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train dataset\n",
      "Generating validation dataset\n",
      "Generating test dataset\n",
      "Model loading: \n",
      "training start:\n",
      "Epoch 1/10, Train Loss: 8.1337, Val Loss: 8.0032\n",
      "Epoch 2/10, Train Loss: 7.7860, Val Loss: 8.0640\n",
      "Epoch 3/10, Train Loss: 7.4800, Val Loss: 8.1679\n",
      "Epoch 4/10, Train Loss: 7.1615, Val Loss: 8.2817\n",
      "Epoch 5/10, Train Loss: 6.8121, Val Loss: 8.3575\n",
      "Epoch 6/10, Train Loss: 6.4209, Val Loss: 8.4447\n",
      "Epoch 7/10, Train Loss: 5.8765, Val Loss: 8.4429\n",
      "Epoch 8/10, Train Loss: 5.0966, Val Loss: 8.5105\n",
      "Epoch 9/10, Train Loss: 4.2410, Val Loss: 8.6174\n",
      "Epoch 10/10, Train Loss: 3.3402, Val Loss: 8.6813\n",
      "------------------------------------\n",
      "Final Test Loss: 8.6813\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    train_corpus = corpus(\"dev\")  #train corpus is too big so using dev corpus for train and test corpus for validation\n",
    "\n",
    "    val_corpus = corpus(\"test\")\n",
    "\n",
    "    # test_corpus = corpus(\"test\")\n",
    "    \n",
    "    model, train_dataset = main(train_corpus, val_corpus, val_corpus)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving model\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    model_path = r\"models\"\n",
    "    curr_notebook_dir = os.getcwd()\n",
    "    parent_dir = os.path.abspath(os.path.join(curr_notebook_dir,os.pardir))\n",
    "    model_path_whole = os.path.join(parent_dir,model_path)\n",
    "    \n",
    "    torch.save(model,os.path.join(model_path_whole,\"speech_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model prediction\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    randtestidx = np.random.randint(0, len(val_corpus))\n",
    "    test_data = val_corpus[randtestidx]\n",
    "    get_predictions(test_data, model, train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
