{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import librosa\n",
    "from python_speech_features import mfcc as psf_mfcc\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tiktoken\n",
    "import json\n",
    "import os\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, filename=\"model_test.log\", filemode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_mfcc(audio_path, n_mfcc=13):\n",
    "#     y, sr = librosa.load(audio_path, sr=None)\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "#     mfcc = torch.tensor(mfcc).unsqueeze(0)  # Add batch dimension\n",
    "#     return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset class for loading and processing speech audio data and corresponding transcripts.\n",
    "\n",
    "    Args:\n",
    "        audio_paths (list of str): List of file paths to the audio files.\n",
    "        transcripts (list of str): List of transcripts corresponding to the audio files.\n",
    "        target_num_frames (int, optional): Target number of frames for the MFCC features. Defaults to 100.\n",
    "        transform (callable, optional): Optional transform to be applied on an audio file. Defaults to None.\n",
    "\n",
    "    Methods:\n",
    "        __len__(): Returns the number of samples in the dataset.\n",
    "        __getitem__(idx): Loads and returns the MFCC features and encoded transcript for the given index.\n",
    "        get_num_classes(): Returns the number of unique classes (transcripts) in the dataset.\n",
    "        decode(encoded_label): Decodes an encoded label back to its original transcript.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - torch.Tensor: The MFCC features of shape (1, num_frames, num_mfcc_coeffs).\n",
    "            - torch.Tensor: The encoded label as a long tensor.\n",
    "\n",
    "    Example:\n",
    "        >>> dataset = SpeechDataset(audio_paths, transcripts, target_num_frames=100)\n",
    "        >>> mfcc_features, label = dataset[0]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, audio_paths, transcripts, target_num_frames=100, transform=None):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.transcripts = transcripts\n",
    "        self.target_num_frames = target_num_frames\n",
    "        self.transform = transform\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(transcripts)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        transcript = self.transcripts[idx]\n",
    "        audio_data, sample_rate = sf.read(audio_path)\n",
    "        if self.transform:\n",
    "            mfcc = self.transform(audio_path)\n",
    "        else:\n",
    "            mfcc = psf_mfcc(audio_data, samplerate=sample_rate, numcep=13)\n",
    "            mfccs_normalized = (mfcc - np.mean(mfcc, axis=0)) / np.std(mfcc, axis=0)\n",
    "        \n",
    "        # Ensure equal number of frames\n",
    "        if mfccs_normalized.shape[0] < self.target_num_frames:\n",
    "            # Pad with zeros\n",
    "            padding = np.zeros((self.target_num_frames - mfccs_normalized.shape[0], mfccs_normalized.shape[1]))\n",
    "            mfccs_normalized = np.vstack((mfccs_normalized, padding))\n",
    "        elif mfccs_normalized.shape[0] > self.target_num_frames:\n",
    "            # Truncate\n",
    "            mfccs_normalized = mfccs_normalized[:self.target_num_frames, :]\n",
    "        \n",
    "        # Adding a channel dimension for CNN input: (num_frames, num_mfcc_coeffs) -> (1, num_frames, num_mfcc_coeffs)\n",
    "        mfccs_normalized = mfccs_normalized[np.newaxis, ...]\n",
    "        \n",
    "        label = self.label_encoder.transform([transcript])[0]\n",
    "        \n",
    "        return torch.tensor(mfccs_normalized, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return len(self.label_encoder.classes_)\n",
    "    \n",
    "    def decode(self, encoded_label):\n",
    "        return self.label_encoder.inverse_transform([encoded_label])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model that combines Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) \n",
    "    for processing and classifying sequences of MFCC features extracted from speech audio.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of target classes for the classification.\n",
    "        num_frames (int): Number of frames in the input MFCC features.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Defines the forward pass of the model.\n",
    "\n",
    "    Example:\n",
    "        >>> model = CNNBiLSTM(num_classes=10, num_frames=100)\n",
    "        >>> inputs = torch.randn(32, 1, 100, 13)  # Example input tensor with batch size 32\n",
    "        >>> outputs = model(inputs)  # Forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, num_frames):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: (batch_size, 32, num_frames//2, num_features//2)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),   # Output: (batch_size, 64, num_frames//4, num_features//4)\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Compute the flattened size after CNN layers\n",
    "        num_features = 13\n",
    "        cnn_output_size = 64 * (num_frames // 4) * (num_features // 4)\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_size=cnn_output_size, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(128 * 2, num_classes)\n",
    "        self.layer_norm = nn.LayerNorm(128 * 2)  # Apply LayerNorm after LSTM output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, 1, num_frames, num_mfcc_coeffs).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        x = self.cnn(x)\n",
    "        x = x.unsqueeze(1)  # Add time dimension for LSTM\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Trains the given model for one epoch using the provided dataloader, loss function, and optimizer.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to be trained.\n",
    "        dataloader (DataLoader): DataLoader providing the training data.\n",
    "        criterion (nn.Module): Loss function to be used for training.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer to be used for updating the model parameters.\n",
    "        device (torch.device): Device on which the model and data should be loaded (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        float: The average training loss for the epoch.\n",
    "\n",
    "    Example:\n",
    "        >>> model = CNNBiLSTM(num_classes=10, num_frames=100).to(device)\n",
    "        >>> criterion = nn.CrossEntropyLoss()\n",
    "        >>> optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "        >>> dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        >>> device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        >>> train_loss = train(model, dataloader, criterion, optimizer, device)\n",
    "        >>> print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the given model on the validation dataset using the provided dataloader and loss function.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to be evaluated.\n",
    "        dataloader (DataLoader): DataLoader providing the validation data.\n",
    "        criterion (nn.Module): Loss function to be used for evaluation.\n",
    "        device (torch.device): Device on which the model and data should be loaded (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        float: The average validation loss for the epoch.\n",
    "\n",
    "    Example:\n",
    "        >>> model = CNNBiLSTM(num_classes=10, num_frames=100).to(device)\n",
    "        >>> criterion = nn.CrossEntropyLoss()\n",
    "        >>> dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "        >>> device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        >>> val_loss = validate(model, dataloader, criterion, device)\n",
    "        >>> print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(mode=\"train\"):\n",
    "    \"\"\"\n",
    "    Load and return the corpus data from a JSON file based on the specified mode.\n",
    "\n",
    "    Args:\n",
    "        mode (str): The mode of the corpus to load. Options are \"train\", \"dev\", or \"test\". \n",
    "                    Default is \"train\".\n",
    "\n",
    "    Returns:\n",
    "        list of dict: A list of dictionaries, each containing the following keys:\n",
    "            - \"name\": The name of the file.\n",
    "            - \"file\": The file path.\n",
    "            - \"transcript\": The transcript of the audio file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the JSON file for the specified mode is not found.\n",
    "        json.JSONDecodeError: If the JSON file is not properly formatted.\n",
    "    \"\"\"\n",
    "    corpus_path = r\"data\\processed\"\n",
    "    curr_notebook_dir = os.getcwd()\n",
    "    parent_dir = os.path.abspath(os.path.join(curr_notebook_dir, os.pardir))\n",
    "    corpus_path_whole = os.path.join(parent_dir, corpus_path)\n",
    "    \n",
    "    mode_path = mode + \"_corpus_new.json\"\n",
    "    \n",
    "    corpus_path_mode = os.path.join(corpus_path_whole, mode_path)\n",
    "    \n",
    "    with open(corpus_path_mode, \"r\") as f:\n",
    "        mode_corpus = json.load(f)\n",
    "    \n",
    "    return mode_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_corpus: list[dict], val_corpus: list[dict], test_corpus: list[dict], device, mode=\"training\"):\n",
    "    \"\"\"\n",
    "    Main function to train or load a CNN-BiLSTM model for speech recognition.\n",
    "\n",
    "    Args:\n",
    "        train_corpus (list of dict): List of dictionaries containing training data paths and transcripts.\n",
    "        val_corpus (list of dict): List of dictionaries containing validation data paths and transcripts.\n",
    "        test_corpus (list of dict): List of dictionaries containing test data paths and transcripts.\n",
    "        device (torch.device): Device on which to run the model (e.g., 'cuda' or 'cpu').\n",
    "        mode (str): Mode of operation. Can be either \"training\" or \"inference\". Default is \"training\".\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): Trained or loaded CNN-BiLSTM model.\n",
    "        train_dataset (SpeechDataset): Training dataset instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_frames = 1000\n",
    "    print(\"Generating train dataset\")\n",
    "    \n",
    "    train_audio_path, train_transcript = [], []\n",
    "    for files in train_corpus:\n",
    "        train_audio_path.append(files[\"file\"])\n",
    "        train_transcript.append(files[\"transcript\"])\n",
    "    train_dataset = SpeechDataset(train_audio_path, train_transcript, target_num_frames=num_frames)\n",
    "    \n",
    "    num_classes = train_dataset.get_num_classes()\n",
    "    \n",
    "    print(\"Generating validation dataset\")\n",
    "    \n",
    "    val_audio_path, val_transcript = [], []\n",
    "    for files in val_corpus:\n",
    "        val_audio_path.append(files[\"file\"])\n",
    "        val_transcript.append(files[\"transcript\"])\n",
    "    val_dataset = SpeechDataset(val_audio_path, val_transcript, target_num_frames=num_frames)\n",
    "    \n",
    "    print(\"Generating test dataset\")\n",
    "    \n",
    "    test_audio_path, test_transcript = [], []\n",
    "    for files in test_corpus:\n",
    "        test_audio_path.append(files[\"file\"])\n",
    "        test_transcript.append(files[\"transcript\"])\n",
    "    test_dataset = SpeechDataset(test_audio_path, test_transcript, target_num_frames=num_frames)\n",
    "    \n",
    "    batch_size = 64\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"Model loading: \")\n",
    "    \n",
    "    if mode == \"training\":\n",
    "        model = CNNBiLSTM(num_classes=num_classes, num_frames=num_frames).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "        print(\"Training start:\")\n",
    "        \n",
    "        num_epochs = 10\n",
    "        for epoch in range(num_epochs):\n",
    "            logging.info(f\"Entered epoch: {epoch}\")\n",
    "            train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "            logging.info(f\"Train loss calculated for epoch {epoch}: {train_loss}\")\n",
    "            val_loss = validate(model, val_dataloader, criterion, device)\n",
    "            logging.info(f\"Val loss calculated for epoch {epoch}: {val_loss}\")\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        test_loss = validate(model, test_dataloader, criterion, device)\n",
    "        print(\"------------------------------------\")\n",
    "        print(f'Final Test Loss: {test_loss:.4f}')\n",
    "        print(\"------------------------------------\")\n",
    "    \n",
    "    elif mode == \"inference\":\n",
    "        model_path = r\"models\"\n",
    "        curr_notebook_dir = os.getcwd()\n",
    "        parent_dir = os.path.abspath(os.path.join(curr_notebook_dir, os.pardir))\n",
    "        model_path_whole = os.path.join(parent_dir, model_path)\n",
    "        \n",
    "        model = torch.load(os.path.join(model_path_whole, \"speech_model.pt\"))\n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(\"Model loaded!\")\n",
    "        print(\"------------------------------------\")\n",
    "    \n",
    "    return model, train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(test_audio, model, train_dataset, device):\n",
    "    \"\"\"\n",
    "    Get the predicted transcript for a given audio file using a trained model.\n",
    "\n",
    "    Args:\n",
    "        test_audio (dict): A dictionary containing the file path and transcript of the test audio.\n",
    "            Example: {\"file\": \"path/to/audio.wav\", \"transcript\": \"actual transcript\"}\n",
    "        model (torch.nn.Module): The trained CNN-BiLSTM model.\n",
    "        train_dataset (SpeechDataset): The dataset instance used during training, required for decoding labels.\n",
    "        device (torch.device): The device to run the model on (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        None: This function prints the original and predicted transcripts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a SpeechDataset instance for the single test audio\n",
    "    test_audio_path = [test_audio[\"file\"]]\n",
    "    test_transcript = [test_audio[\"transcript\"]]\n",
    "    test_dataset = SpeechDataset(test_audio_path, test_transcript, target_num_frames=1000)\n",
    "    \n",
    "    inputs, label = test_dataset[0]\n",
    "    inputs = inputs.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.argmax(outputs, 1).item()\n",
    "    \n",
    "    predicted_transcript = train_dataset.decode(predicted)\n",
    "    \n",
    "    print(f\"Original transcript: {test_transcript[0]}\")\n",
    "    print(f\"Predicted transcript: {predicted_transcript}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    train_corpus = corpus(\"dev\")  #train corpus is too big so using dev corpus for train and test corpus for validation\n",
    "\n",
    "    val_corpus = corpus(\"test\")\n",
    "\n",
    "    # test_corpus = corpus(\"test\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train dataset\n",
      "Generating validation dataset\n",
      "Generating test dataset\n",
      "Model loading: \n",
      "Model loaded!\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#model training or inference\n",
    "if __name__ == \"__main__\":\n",
    "    model, train_dataset = main(train_corpus, val_corpus, val_corpus, device, mode=\"inference\")  \n",
    "    #mode can be \"inference\" for already trained models to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model saving for training_mode\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    model_path = r\"models\"\n",
    "    curr_notebook_dir = os.getcwd()\n",
    "    parent_dir = os.path.abspath(os.path.join(curr_notebook_dir,os.pardir))\n",
    "    model_path_whole = os.path.join(parent_dir,model_path)\n",
    "    \n",
    "    torch.save(model,os.path.join(model_path_whole,\"speech_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original transcript: I ALMOST THINK I CAN REMEMBER FEELING A LITTLE DIFFERENT\n",
      "Predicted transcript: BOTH LIPS ASKED THE LARVAE\n"
     ]
    }
   ],
   "source": [
    "#Testing model prediction\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    randtestidx = np.random.randint(0, len(val_corpus))\n",
    "    test_data = val_corpus[randtestidx]\n",
    "    get_predictions(test_data, model, train_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
